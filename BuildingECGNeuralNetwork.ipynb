{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa5f80a0-bf43-4c71-a9de-8111b32eee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io \n",
    "import wfdb\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9919bc-2b63-452f-a0e0-90a3c21b5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the custom denoising functions that I wrote\n",
    "from utils.denoising_functions import Nvar_Calculation_from_std_dev, NLM_1dDarbon, NLM_1dDarbon_2D_full\n",
    "from utils.denoising_functions import Butterworth_lowpass_filter, remove_baseline_loess\n",
    "from utils.PlottingFunctions import quick_plot, plot_ecg_stacked\n",
    "from utils.ExtractingGroups import extract_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89af8263-ff88-4f1d-af7a-11b9bcfe3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the tensorflow package and layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, ReLU, Add, Input\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc5a2237-37e3-49e7-8037-7fa86853c581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Original WFDB folder\n",
    "base_path = \"/home/lewisa13/ECG_Data/physionet.org/files/ecg-arrhythmia/1.0.0/WFDBRecords\"\n",
    "output_path = \"CleanedECG\" # Folder to save denoised signal\n",
    "\n",
    "os.makedirs(output_path, exist_ok = True)\n",
    "\n",
    "fs = 500 # Sampling frequency is 500 Hz\n",
    "cutoff = 50 #Setting a cutoff of 50 Hz for low pass filtering\n",
    "\n",
    "## NLM Denoising Parameters\n",
    "# PatchHW is the patch half width\n",
    "#Patch length = 2* PatchHW +1\n",
    "#At 500 Hz sampling, 1 sample = 2ms, the QRS complex is about 40-60 samples\n",
    "# Want the patch length to be less than QRS comples, so between 10-20 samples\n",
    "# PatchLength used in the paper was 10, So recommened PatchHW is 5\n",
    "PatchHW = 5\n",
    "\n",
    "# P is the search window, controls how far the algorithm searches for similar patches\n",
    "P = 25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ad100a-b6e0-41d1-9cae-9a6458bfb421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lewisa13/ECG_Data/physionet.org/files/ecg-arrhythmia/1.0.0/WFDBRecords\n",
      "Found 45152 .mat files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   3 out of  10 | elapsed:    8.9s remaining:   20.8s\n",
      "[Parallel(n_jobs=16)]: Done   5 out of  10 | elapsed:    9.0s remaining:    9.0s\n",
      "[Parallel(n_jobs=16)]: Done   7 out of  10 | elapsed:    9.1s remaining:    3.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 9.7 s (0.2 min)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done  10 out of  10 | elapsed:    9.6s finished\n",
      "UsageError: Line magic function `%print(\"All` not found.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Function to process one file\n",
    "# --------------------------\n",
    "def process_ecg_file(mat_path, fs, cutoff, P, PatchHW):\n",
    "    record_id = os.path.splitext(os.path.basename(mat_path))[0]\n",
    "    output_file = os.path.join(save_folder, f\"{record_id}.npy\")\n",
    "\n",
    "    # Skip if already processed\n",
    "    if os.path.exists(output_file):\n",
    "        print(\"Skipping:\", record_id)\n",
    "        return None\n",
    "\n",
    "    print(\"Processing:\", record_id)\n",
    "\n",
    "    header_path = mat_path.replace(\".mat\", \".hea\")\n",
    "\n",
    "    # --------------------------\n",
    "    # Load ECG\n",
    "    # --------------------------\n",
    "    mat_data = scipy.io.loadmat(mat_path)\n",
    "    ecg = mat_data['val']  # shape (n_leads, n_samples)\n",
    "\n",
    "    # --------------------------\n",
    "    # Butterworth low-pass filter\n",
    "    # --------------------------\n",
    "    ecg_low_pass = Butterworth_lowpass_filter(ecg, fs, cutoff, order=2)\n",
    "\n",
    "    # --------------------------\n",
    "    # LOESS baseline removal (threading-safe)\n",
    "    # --------------------------\n",
    "    n_leads = ecg_low_pass.shape[0]\n",
    "\n",
    "    \n",
    "    ecg_loess = np.array([\n",
    "        remove_baseline_loess(ecg_low_pass[ilead, :], fs, frac=0.05, it=3)\n",
    "        for ilead in range(n_leads)\n",
    "    ])\n",
    "\n",
    "\n",
    "    # --------------------------\n",
    "    # Compute noise variance for NLM\n",
    "    # --------------------------\n",
    "    Nvar = Nvar_Calculation_from_std_dev(ecg_loess)\n",
    "\n",
    "    # --------------------------\n",
    "    # NLM denoising (non-vectorized per lead)\n",
    "    # --------------------------\n",
    "    ecg_denoised = np.array([\n",
    "        NLM_1dDarbon(ecg_loess[ilead, :], Nvar, P, PatchHW)\n",
    "        for ilead in range(n_leads)\n",
    "    ])\n",
    "\n",
    "    # plot_ecg_stacked(ecg_denoised, fs=500, n_signals=5, lead_names=None,spacing_factor=2, linewidth=1)\n",
    "    # fig = plt.gcf()\n",
    "    # fig.savefig(output_plot_path)\n",
    "    # plt.close(fig)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Save denoised ECG\n",
    "    # --------------------------\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    np.save(os.path.join(save_folder, f\"{record_id}.npy\"), ecg_denoised)\n",
    "    print(\"Saved:\", record_id)\n",
    "    \n",
    "    # --------------------------\n",
    "    # Extract grouedp labels\n",
    "    # --------------------------\n",
    "    group_code = extract_group(header_path)\n",
    "    label_row = [record_id, \"|\".join(group_code)]\n",
    "\n",
    "    return label_row\n",
    "\n",
    "# --------------------------\n",
    "# Main function to process all .mat files\n",
    "# --------------------------\n",
    "def process_all_ecg(base_path, fs, cutoff, P, PatchHW):\n",
    "    # Gather all .mat files recursively\n",
    "    print(base_path)\n",
    "    all_mat_files = [\n",
    "        os.path.join(root, f)\n",
    "        for root, dirs, files in os.walk(base_path)\n",
    "        for f in files\n",
    "        if f.endswith(\".mat\")\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(all_mat_files)} .mat files\")\n",
    "\n",
    "    #Selecting a few test files for now. Working on a code to process the files batchwise\n",
    "    test_files = all_mat_files[110:120]\n",
    "    \n",
    "    with parallel_backend(\"loky\",n_jobs=16):\n",
    "        label_rows = Parallel(verbose=10)(\n",
    "            delayed(process_ecg_file)(mat_path, fs, cutoff, P, PatchHW)\n",
    "            for mat_path in test_files\n",
    "    )\n",
    "\n",
    "    # Process files in parallel (threading backend for debugger-safe)\n",
    "    # with parallel_backend('threading', n_jobs=-1):\n",
    "    #     label_rows = Parallel()(\n",
    "    #         delayed(process_ecg_file)(mat_path, fs, cutoff, P, PatchHW)\n",
    "    #         for mat_path in all_mat_files\n",
    "    #     )\n",
    "\n",
    "    # label_rows = [\n",
    "    #     process_ecg_file(mat_path, fs, cutoff, P, PatchHW)\n",
    "    #     for mat_path in all_mat_files[100:105]\n",
    "    # ]\n",
    "    \n",
    "    return label_rows\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Usage\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    base_path =  \"/home/lewisa13/ECG_Data/physionet.org/files/ecg-arrhythmia/1.0.0/WFDBRecords\"\n",
    "    save_folder = \"/home/lewisa13/CleanedECG\"\n",
    "    output_plot_path = \"/home/lewisa13/CleanedECGPlots\"\n",
    "    fs = 500          # sampling frequency\n",
    "    cutoff = 40       # low-pass cutoff\n",
    "    P = 20            # NLM search window\n",
    "    PatchHW = 3       # NLM patch half-width\n",
    "\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    labels = process_all_ecg(base_path, fs, cutoff, P, PatchHW)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    runtime = end - start\n",
    "    print(f\"Total runtime: {runtime:.1f} s ({runtime/60:.1f} min)\")\n",
    "    %print(\"All files processed. Label rows:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36b03a33-6ea4-4314-8d22-fb155994116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: JS00117\n",
      "Saved: JS00117\n",
      "Processing: JS00115\n",
      "Saved: JS00115\n",
      "Processing: JS00121\n",
      "Saved: JS00121\n",
      "Processing: JS00118\n",
      "Saved: JS00118\n",
      "Processing: JS00124\n",
      "Saved: JS00124\n",
      "Processing: JS00122\n",
      "Saved: JS00122\n",
      "Processing: JS00123\n",
      "Saved: JS00123\n",
      "Processing: JS00120\n",
      "Saved: JS00120\n",
      "Processing: JS00119\n",
      "Saved: JS00119\n",
      "Processing: JS00116\n",
      "Saved: JS00116\n"
     ]
    }
   ],
   "source": [
    "# Open a new csv file to write the group label for each ECG\n",
    "# If the file exists, it overwrites it, if ti doesn't exist it creates a new file\n",
    "# newline= \"\" ensures that python doesn't insert extra blank lines when writing row on windows\n",
    "# open automatically closes the file when done\n",
    "with open(\"labels.csv\",\"w\",newline = \"\") as f:\n",
    "    #Create a write object that lets you writerows to the CSV file easily\n",
    "    writer = csv.writer(f)\n",
    "    #.writerow wrtes a single row to the CSV file. Here it is the header row\n",
    "    writer.writerow([\"record_id\",\"group_code\"])\n",
    "    # .writerows writes multiple rows at once\n",
    "    writer.writerows(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de41a581-688e-43de-8847-a30080d5c93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A|F|I|B' 'S|B']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"labels.csv\")\n",
    "\n",
    "le = LabelEncoder() # LabelEncoder is a tool from scikit-learn used to convert categorical labesl \n",
    "y = le.fit_transform(df[\"group_code\"])\n",
    "\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09ed39-454c-4139-b86e-0bbf3a7b88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the intial block\n",
    "\n",
    "def initialConvBlock(input):\n",
    "    # Defines the inital block of the neural network\n",
    "    #First block has 64 filters, filter length of 16 and stride of 4, \"same' is added to determine the required padding\n",
    "    x1 = Conv1D(filters = 64, kernel_size = 16,padding = 'same',strides = 4)(input)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = ReLU()(x1)\n",
    "\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc23679-a9d2-45bc-9560-efeb265afea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the resblk block using a function, N_filters is used as an input since the filters double ever two res blocks\n",
    "\n",
    "def resblk(input, N_filters, stride_size, dropout_rate):\n",
    "    # resblk has one input which splits into two branches coming from the previous layer\n",
    "\n",
    "    #Max pooling branch is the first branch\n",
    "    x1 = MaxPooling1D(pool_size = 4, strides = stride_size, padding = \"valid\")(input)\n",
    "    x1 = Conv1D(filters = N_filters, kernel_size = 1, padding = \"valid\")(x1)\n",
    "\n",
    "    # Conv layer is the second branch \n",
    "    x2 = Conv1D(filters = N_filters, kernel_size = 16, strides = stride_size, padding = \"same\")(input)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = ReLU()(x2)\n",
    "    x2 = Dropout(dropout_rate)(x2)\n",
    "\n",
    "    x2 = Conv1D(filters = N_filters, kernel_size = 16, strides = 1, padding = \"same\")(x2)\n",
    "\n",
    "    # Add the output from the two branches\n",
    "    x3 = Add()([x1, x2])\n",
    "\n",
    "    # Apply the batch normalization, ReLU and Dropout layers\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = ReLU()(x3)\n",
    "    x3 = Dropout(dropout_rate)(x3)\n",
    "    \n",
    "    return x3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992f089-e651-4254-867b-2e46b98d36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting together the entire network\n",
    "num_classes = 4\n",
    "\n",
    "inputs = Input(shape = (4096,12))\n",
    "\n",
    "convblock1 = initialConvBlock(inputs)\n",
    "\n",
    "resblk1 = resblk(convblock1, N_filters = 64, stride_size = 4, dropout_rate = 0.5)\n",
    "\n",
    "resblk2 = resblk(resblk1, N_filters = 64, stride_size = 4, dropout_rate = 0.5)\n",
    "\n",
    "resblk3 = resblk(resblk2, N_filters = 128, stride_size = 4, dropout_rate = 0.5)\n",
    "\n",
    "resblk4 = resblk(resblk3, N_filters = 192, stride_size = 4, dropout_rate = 0.5)\n",
    "\n",
    "flatten_layer = Flatten()(resblk4)\n",
    "\n",
    "Output_dense_layer = Dense(units = num_classes, activation = 'sigmoid')(flatten_layer)\n",
    "\n",
    "model = Model(inputs= inputs, outputs = Output_dense_layer)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Beta=1.0 makes it a standard F1 score\n",
    "# average='micro' aggregates TPs, FPs, and FNs across all classes\n",
    "micro_f1 = tf.keras.metrics.FBetaScore(beta=1.0, average='micro', threshold=0.5)\n",
    "\n",
    "# precision = tf.keras.Precision()\n",
    "# recall = tf.keras.Recall()\n",
    "optimizer = Adam(learning_rate = 0.001)\n",
    "model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy', micro_f1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971143c-0779-42de-b18a-b44921abf1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba318b-f9c3-49d6-adb5-42302a7f6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
